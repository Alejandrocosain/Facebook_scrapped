{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Profile Page Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to analyze and structure public information located at a public profile page in Facebook using Selenium and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and methods for analyzing html code from websites\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "\n",
    "# Import packages and methods for automating website navigation with selenium and chromedriver\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Process output as json file\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input here the total amount of posts to collect\n",
    "NUM_POSTS = 50\n",
    "\n",
    "# Input the public facebook profile url\n",
    "URL = \"https://www.facebook.com/stevejobsfilm\"\n",
    "\n",
    "# Input chromedriver location\n",
    "CHROMEDRIVER_PATH = '/Users/alex/Downloads/chromedriver'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define support functions for notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform click action on a button and return True, in case of any error, return False\n",
    "def click_expand_button(button):\n",
    "    try:\n",
    "        button.click()\n",
    "    except :\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Perform hover action over date element in a post with a shared related post to load its url and return True if successful, otherwise return False\n",
    "def show_post_url(i):\n",
    "    try:\n",
    "        # Identify date element within a shared post\n",
    "        a = driver.find_element(By.CSS_SELECTOR, 'div.x9f619.x1n2onr6.x1ja2u2z.xeuugli.x1iyjqo2.xs83m0k.x1xmf6yo.x1emribx.x1e56ztr.x1i64zmx.xjl7jj.x19h7ccj.x65f84u')\\\n",
    "                  .find_elements(By.CSS_SELECTOR,'div.x9f619.x1n2onr6.x1ja2u2z.x1jx94hy.x1qpq9i9.xdney7k.xu5ydu1.xt3gfkd.xh8yej3.x6ikm8r.x10wlt62.xquyuld')[i]\\\n",
    "                  .find_element(By.CSS_SELECTOR,'div.x1a8lsjc.x1swvt13.x1pi30zi')\\\n",
    "                  .find_element(By.CSS_SELECTOR,'a.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx.x1sy0etr.x17r0tee.x972fbf.xcfux6l.x1qhh985.xm0m39n.x9f619.x1ypdohk.xt0psk2.xe8uvvx.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x16tdsg8.x1hl2dhg.xggy1nq.x1a2a7pz.x1sur9pj.xkrqix3.xi81zsa')\n",
    "       \n",
    "        # If element is located, move browser to element\n",
    "        ActionChains(driver).move_to_element(a).perform()\n",
    "        \n",
    "        # Wait a second for html to update url\n",
    "        time.sleep(1)\n",
    "        \n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run scrapping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 posts loaded\n",
      "10 posts loaded\n",
      "14 posts loaded\n",
      "19 posts loaded\n",
      "22 posts loaded\n",
      "25 posts loaded\n",
      "29 posts loaded\n",
      "33 posts loaded\n",
      "37 posts loaded\n",
      "40 posts loaded\n",
      "44 posts loaded\n",
      "47 posts loaded\n",
      "51 posts loaded\n",
      "post 0 has 0 load buttons\n",
      "post 1 has 1 load buttons\n",
      "post 2 has 0 load buttons\n",
      "post 3 has 1 load buttons\n",
      "post 4 has 0 load buttons\n",
      "post 5 has 0 load buttons\n",
      "post 6 has 0 load buttons\n",
      "post 7 has 0 load buttons\n",
      "post 8 has 0 load buttons\n",
      "post 9 has 0 load buttons\n",
      "post 10 has 0 load buttons\n",
      "post 11 has 1 load buttons\n",
      "post 12 has 0 load buttons\n",
      "post 13 has 0 load buttons\n",
      "post 14 has 0 load buttons\n",
      "post 15 has 0 load buttons\n",
      "post 16 has 0 load buttons\n",
      "post 17 has 0 load buttons\n",
      "post 18 has 0 load buttons\n",
      "post 19 has 0 load buttons\n",
      "post 20 has 0 load buttons\n",
      "post 21 has 0 load buttons\n",
      "post 22 has 0 load buttons\n",
      "post 23 has 0 load buttons\n",
      "post 24 has 0 load buttons\n",
      "post 25 has 0 load buttons\n",
      "post 26 has 0 load buttons\n",
      "post 27 has 0 load buttons\n",
      "post 28 has 0 load buttons\n",
      "post 29 has 0 load buttons\n",
      "post 30 has 0 load buttons\n",
      "post 31 has 0 load buttons\n",
      "post 32 has 0 load buttons\n",
      "post 33 has 0 load buttons\n",
      "post 34 has 0 load buttons\n",
      "post 35 has 0 load buttons\n",
      "post 36 has 0 load buttons\n",
      "post 37 has 0 load buttons\n",
      "post 38 has 0 load buttons\n",
      "post 39 has 0 load buttons\n",
      "post 40 has 0 load buttons\n",
      "post 41 has 0 load buttons\n",
      "post 42 has 0 load buttons\n",
      "post 43 has 0 load buttons\n",
      "post 44 has 1 load buttons\n",
      "post 45 has 0 load buttons\n",
      "post 46 has 0 load buttons\n",
      "post 47 has 0 load buttons\n",
      "post 48 has 0 load buttons\n",
      "post 49 has 0 load buttons\n",
      "post 50 has 0 load buttons\n"
     ]
    }
   ],
   "source": [
    "# Set up headless execution\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "\n",
    "# Initialize chromedriver and maximize window \n",
    "driver = webdriver.Chrome(executable_path = CHROMEDRIVER_PATH, options = chrome_options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# Load Facebook profile url and close sign-up popup\n",
    "driver.get(URL)\n",
    "time.sleep(1)\n",
    "driver.find_element(By.CSS_SELECTOR,\"div.x92rtbv.x10l6tqk.x1tk7jg1.x1vjfegm\").click()\n",
    "time.sleep(1)\n",
    "\n",
    "# Hide sign-up banner at the bottom of the website\n",
    "driver.execute_script('document.querySelector(\"div.x78zum5.xdt5ytf.x2lah0s.x193iq5w.x2bj2ny.x1ey2m1c.xayqjjm.x9f619.xds687c.x1xy6bms.xn6708d.x1s14bel.x1ye3gou.xixxii4.x17qophe.x1u8a7rm\").style.visibility = \"hidden\";')\n",
    "\n",
    "# ------ PROCESS 1: Load all required posts ------\n",
    "\n",
    "# Define control variables for monitoring post loading cycle\n",
    "last_amount_posts = 0 \n",
    "frozen_counter = 0\n",
    "\n",
    "# Go down to the bottom of the webpage until it reaches the number of posts required\n",
    "while len(driver.find_elements(By.CSS_SELECTOR, \"div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z\")) < NUM_POSTS :\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(2)\n",
    "    print(\"{} posts loaded\".format(len(driver.find_elements(By.CSS_SELECTOR, \"div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z\"))))\n",
    "    \n",
    "    # If no new posts were loaded, increase frozen counter by 1\n",
    "    if len(driver.find_elements(By.CSS_SELECTOR, \"div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z\")) <= last_amount_posts:\n",
    "        frozen_counter += 1\n",
    "        \n",
    "        # If no new posts were loaded after 3 attempts, break cycle\n",
    "        if frozen_counter == 3:\n",
    "            break\n",
    "    \n",
    "    # Store new amount of posts loaded\n",
    "    else:\n",
    "        last_amount_posts = len(driver.find_elements(By.CSS_SELECTOR, \"div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z\"))\n",
    "    \n",
    "\n",
    "# Return back to the top of the webpage to prepare for next process\n",
    "driver.execute_script(\"window.scrollTo(0, 0)\")\n",
    "\n",
    "# ------ PROCESS 2: Expand text from all the posts ------\n",
    "# Store object with the css selector identifier for \"show more\" button\n",
    "str_loadmore_button= \"div.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx.x1sy0etr.x17r0tee.x972fbf.xcfux6l.x1qhh985.xm0m39n.x9f619.x1ypdohk.xt0psk2.xe8uvvx.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x16tdsg8.x1hl2dhg.xggy1nq.x1a2a7pz.x1sur9pj.xkrqix3.xzsf02u.x1s688f\"\n",
    "\n",
    "# Iterate for all posts in website\n",
    "for i in range(len(driver.find_elements(By.CSS_SELECTOR, \"div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z\"))):\n",
    "    \n",
    "    # Load post as a selenium element\n",
    "    actual_post = driver.find_elements(By.CSS_SELECTOR, \"div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z\")[i]\n",
    "    \n",
    "    # Look for all \"show more\" buttons\n",
    "    list_loadmore_button = actual_post.find_elements(By.CSS_SELECTOR, str_loadmore_button)\n",
    "    print(\"post {0} has {1} load buttons\".format(i, len(list_loadmore_button)))\n",
    "    \n",
    "    # If at least one button is found, perform click action on them\n",
    "    if len(list_loadmore_button)>0:\n",
    "        for button in list_loadmore_button[:1]:\n",
    "            button.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "# ------ PROCESS 3: Load url of shared posts ------\n",
    "# Initialize empty list for locating posts with shared posts\n",
    "list_shared_url_posts = []    \n",
    "\n",
    "# Initialize actions class for performing actions in chromedriver \n",
    "actions = ActionChains(driver)\n",
    "\n",
    "# Iterate for all posts in website\n",
    "for i in range(len(driver.find_elements(By.CSS_SELECTOR, \"div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z\"))):\n",
    "    \n",
    "    # If the date element of a shared post is located within the post, identify this post as having a shared publication\n",
    "    if show_post_url(i):\n",
    "        print(\"post {} contains shared post\".format(i+1))\n",
    "        list_shared_url_posts.append(i)\n",
    "\n",
    "# ------ STRUCTURE INFORMATION WITH BEAUTIFULSOUP ------    \n",
    "# Scrap html code from website in chromedriver, and close the chromedriver session\n",
    "web_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "driver.quit()\n",
    "\n",
    "# Get general information from profile \n",
    "str_name = web_soup.find(\"h1\",class_=\"html-h1 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x1vvkbs x1heor9g x1qlqyl8 x1pd3egz x1a2a7pz\").next_element.strip()\n",
    "str_followed = web_soup.find(\"div\",class_=\"x9f619 x1n2onr6 x1ja2u2z x78zum5 xdt5ytf x2lah0s x193iq5w x1cy8zhl xyamay9\").next_element()[1].next_element.strip()\n",
    "str_followers = web_soup.find(\"div\",class_=\"x9f619 x1n2onr6 x1ja2u2z x78zum5 xdt5ytf x2lah0s x193iq5w x1cy8zhl xyamay9\").next_element()[0].next_element.strip().replace(\"\\xa0\",\" \").replace(\".\",\"\")\n",
    "str_image_url = web_soup.find(\"svg\",class_=\"x3ajldb\").find(\"image\")[\"xlink:href\"]\n",
    "\n",
    "# Collect all posts from html code, up until NUM_POSTS\n",
    "list_all_post = web_soup.find(\"div\",class_ = \"x9f619 x1n2onr6 x1ja2u2z xeuugli x1iyjqo2 xs83m0k x1xmf6yo x1emribx x1e56ztr x1i64zmx xjl7jj x19h7ccj x65f84u\").find_all(\"div\",class_ = \"x78zum5 x1n2onr6 xh8yej3\")[:NUM_POSTS]\n",
    "\n",
    "# LOOP THROUGH ALL POSTS TO SCRAP INFORMATION\n",
    "# Initialize empty list that will contain posts information\n",
    "list_str_posts = []\n",
    "\n",
    "# Iterate through all posts\n",
    "int_post = 0\n",
    "for post in list_all_post:\n",
    "    \n",
    "    # Store text if text element is located, otherwise indicate that no text is available\n",
    "    if len(post.find_all(\"span\",class_=\"x193iq5w xeuugli x13faqbe x1vvkbs xlh3980 xvmahel x1n0sxbx x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x x4zkp8e x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u x1yc453h\"))>0:\n",
    "        \n",
    "        # Collect all paragraphs in post text \n",
    "        list_all_paragraphs = post.find_all(\"span\",class_=\"x193iq5w xeuugli x13faqbe x1vvkbs xlh3980 xvmahel x1n0sxbx x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x x4zkp8e x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u x1yc453h\")[0].find_all(\"div\",class_=\"xat24cr\")\n",
    "\n",
    "        # Initialize empty string to collect all available text\n",
    "        str_post = \"\"\n",
    "\n",
    "        # Iterate through all fond paragraphs \n",
    "        for paragraph in list_all_paragraphs: \n",
    "            \n",
    "            # Get all elements from paragraphs and iterate\n",
    "            proc_paragraph = paragraph.find(\"div\").contents\n",
    "\n",
    "            for element in proc_paragraph:\n",
    "                \n",
    "                # If element contains only string, append it directly \n",
    "                if type(element) == NavigableString:\n",
    "                    str_post += element\n",
    "\n",
    "                # Otherwise, if tag element contains a string, append that string\n",
    "                elif len(element.find_all(\"a\",class_=\"x1i10hfl\")):\n",
    "                    str_post += element.next_element.next_element\n",
    "\n",
    "            str_post += \"\\n\\n\"\n",
    "    else:\n",
    "        str_post = \"NO TEXT AVAILABLE\"\n",
    "        \n",
    "    # If iterating post was identified to contain a shared post, get the url for that shared post\n",
    "    if int_post in list_shared_url_posts: \n",
    "        str_shared_url = web_soup.find('div', class_ = 'x9f619 x1n2onr6 x1ja2u2z xeuugli x1iyjqo2 xs83m0k x1xmf6yo x1emribx x1e56ztr x1i64zmx xjl7jj x19h7ccj x65f84u').find_all('div', class_ = 'x78zum5 x1n2onr6 xh8yej3')[int_post].find_all('div', class_ = 'x78zum5 xdt5ytf xz62fqu x16ldp7u')[2].find(\"a\",class_=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz x1sur9pj xkrqix3 xi81zsa xo1l8bm\")[\"href\"]\n",
    "    else:\n",
    "        str_shared_url = \"\"\n",
    "    \n",
    "    # Initialize list that will contain urls for all found images\n",
    "    list_url_img = []\n",
    "    \n",
    "    # Look for all images, using class identifiers for single images and multiple images within a post\n",
    "    list_mult_img = post.find_all(\"img\",class_ =\"xz74otr x1ey2m1c xds687c x5yr21d x10l6tqk x17qophe x13vifvy xh8yej3\")\n",
    "    list_sing_img = post.find_all(\"img\",class_ =\"x1ey2m1c xds687c x5yr21d x10l6tqk x17qophe x13vifvy xh8yej3 xl1xv1r\")\n",
    "    \n",
    "    # Add all urls located with the multiple images identifier\n",
    "    if len(list_mult_img) > 0:\n",
    "        list_mult_img_url = [img[\"src\"] for img in list_mult_img]\n",
    "        list_url_img += list_mult_img_url\n",
    "        \n",
    "    # Add all urls located with the single images identifier\n",
    "    if len(list_sing_img) > 0:\n",
    "        list_sing_img_url = [img[\"src\"] for img in list_sing_img]\n",
    "        list_url_img += list_sing_img_url\n",
    "    \n",
    "    # Store dictionary with info scrapped from post\n",
    "    dict_post = {\"text\" : str_post, \"shared_post_url\" : str_shared_url, \"img_url\" : list_url_img}\n",
    "     \n",
    "    # Append dictionary into general list with post information\n",
    "    list_str_posts.append(dict_post)\n",
    "    int_post+=1\n",
    "\n",
    "# Structure scrapped data into a dictionary \n",
    "dict_output = {\"name\":str_name,\"followers\":str_followers,\"followed\":str_followed,\"image_url\":str_image_url,\"recent_posts\":list_str_posts[:NUM_POSTS]}\n",
    "\n",
    "# Transform dictionary into json object\n",
    "json_object = json.dumps(dict_output, indent=4)\n",
    " \n",
    "# Writing to result .json\n",
    "with open(\"result.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
